# -*- coding: utf-8 -*-
"""COVID.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AqP_mwzWaIxJIq-FfBUP5L4Rp4NwyFpE
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
import os
from PIL import Image
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import StepLR
import numpy as np
from PIL import ImageFile
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import numpy as np
import pandas as pd
import os
import random 
from shutil import copyfile
from torch.utils.data import Dataset
from torchvision.datasets import ImageFolder
from PIL import Image
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import re
import albumentations as albu
from albumentations.pytorch import ToTensor
!pip install torchxrayvision
import torchxrayvision as xrv
from sklearn.metrics import roc_auc_score

get_ipython().system('pip install --upgrade efficientnet-pytorch')

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
train_transformer = transforms.Compose([
   transforms.Resize(256),
   transforms.RandomResizedCrop((224), scale=(0.5,1.0)),
   transforms.RandomHorizontalFlip(),
   transforms.ToTensor(),
   normalize                                    
])

val_transformer = transforms.Compose([
    transforms.Resize(224),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    normalize
])

batchsize = 10
def read_txt(txt_path):
  with open(txt_path) as f:
    lines = f.readlines()
  txt_data = [line.strip() for line in lines]
  return (txt_data)

class COVID(Dataset):
  def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):
    self.transform = transform
    self.root_dir = root_dir
    self.txt_path = [txt_COVID, txt_NonCOVID]
    self.classes = ['CT_COVID', 'CT_NonCOVID']
    self.num_cls = len(self.classes)
    self.img_list = []
    for c in range(self.num_cls):
      cls_list = [[os.path.join(self.root_dir, self.classes[c], item), c] for item in read_txt(self.txt_path[c])]
      self.img_list += cls_list

  def __len__(self):
    return len(self.img_list)

  def __getitem__(self, idx):
    if torch.is_tensor(idx):
      idx = idx.tolist()
    target = self.img_list[idx][1]

    img_path = self.img_list[idx][0]
    img = Image.open(img_path).convert('RGB')
    if self.transform:
      img = self.transform(img)
    return img, target

if __name__ == '__main__':
  trainset = COVID(root_dir='gdrive/My Drive/Projects/COVID/Datasets',
                            txt_COVID='/content/gdrive/My Drive/Projects/COVID/Datasets/Data-split/COVID/trainCT_COVID.txt',
                            txt_NonCOVID='/content/gdrive/My Drive/Projects/COVID/Datasets/Data-split/NonCOVID/trainCT_NonCOVID.txt',
                            transform= train_transformer)
  """valset = CovidCTDataset(root_dir='new_data/4.4_image',
                              txt_COVID='new_data/newtxt/val.txt',
                              txt_NonCOVID='old_data/oldtxt/valCT_NonCOVID.txt',
                              transform= val_transformer)"""
  testset = COVID(root_dir='gdrive/My Drive/Projects/COVID/Datasets',
                              txt_COVID='/content/gdrive/My Drive/Projects/COVID/Datasets/Data-split/COVID/testCT_COVID.txt',
                              txt_NonCOVID='/content/gdrive/My Drive/Projects/COVID/Datasets/Data-split/NonCOVID/testCT_NonCOVID.txt',
                              transform= val_transformer)
  print(trainset.__len__())
  #print(valset.__len__())
  print(testset.__len__())


  train_loader = DataLoader(trainset, batch_size=batchsize, shuffle=True)
  #val_loader = DataLoader(valset, bat_size=batchsize, drop_last=False, shuffle=False)
  test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)

class DenseNet(nn.Module):
  def __init__(self):
    super(DenseNet, self).__init__()
    self.dense_net = xrv.models.DenseNet(num_classes=2, in_channels=3)
    self.criterion = nn.CrossEntropyLoss()


  def forward(self, x):
    logits = self.dense_net(x)
    return logits

model = DenseNet()
modelname = 'DenseNet_medical'

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)

criteria = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum = 0.9)
loss_list = []
acc_list = []
val_list = []
valloss_list = []
F1_list = []
AUC_list = []
acc_list = []
votenum = 10

torch.load(model.state_dict(), "/content/gdrive/My Drive/Projects/COVID/model")

vote_pred = np.zeros(testset.__len__())
vote_score = np.zeros(testset.__len__())

for e in range(150):
  loss_net= 0
  acc_net = 0
  val_acc = 0
  val_loss = 0
  total = 0
  total1 = 0
  TP = 0
  TN = 0
  FN = 0
  FP = 0
  model.train()
  for images, labels in train_loader:
    images = images.to(device)
    labels = labels.to(device)
    pred = model(images)
    loss = criteria(pred,labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    _, pred = torch.max(pred.data,1)
    loss_net += loss.item()
    acc_net += pred.eq(labels.data).sum()
    total += labels.size(0)
  
  else:
    model.eval()
    with torch.no_grad():
      predlist=[]
      scorelist=[]
      targetlist=[]
      for images, labels in test_loader:
        images = images.to(device)
        labels = labels.to(device)
        pred = model(images)
        score = F.softmax(pred, dim=1)
        scorelist=np.append(scorelist, pred.cpu().numpy()[:,1])
        loss_val = criteria(pred,labels)
        _, pred = torch.max(pred.data,1)
        val_loss += loss_val.item()
        val_acc += pred.eq(labels.data).sum()
        total1 += labels.size(0)
        targetcpu=labels.long().cpu().numpy()
        predlist=np.append(predlist, pred.cpu().numpy())
        targetlist=np.append(targetlist,targetcpu)
  vote_pred = vote_pred + predlist 
  vote_score = vote_score + scorelist 

  if e % votenum == 0:
        
        # major vote
      vote_pred[vote_pred <= (votenum/2)] = 0
      vote_pred[vote_pred > (votenum/2)] = 1
      vote_score = vote_score/votenum
        
      #print('vote_pred', vote_pred)
      #print('targetlist', targetlist)
      TP = ((vote_pred == 1) & (targetlist == 1)).sum()
      TN = ((vote_pred == 0) & (targetlist == 0)).sum()
      FN = ((vote_pred == 0) & (targetlist == 1)).sum()
      FP = ((vote_pred == 1) & (targetlist == 0)).sum()
        
        
      #print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)
      #print('TP+FP',TP+FP)
      p = TP / (TP + FP)
      #print('precision',p)
      p = TP / (TP + FP)
      r = TP / (TP + FN)
      #print('recall',r)
      F1 = 2 * r * p / (r + p)
      acc = (TP + TN) / (TP + TN + FP + FN)
      print('F1',F1)
      print('acc',acc)
      AUC = roc_auc_score(targetlist, vote_score)
      #print('AUCp', roc_auc_score(targetlist, vote_pred))
      print('AUC', AUC)
      vote_pred = np.zeros(testset.__len__())
      vote_score = np.zeros(testset.__len__())
  val_loss /= total1
  valloss_list.append(val_loss)
  val_acc = (int(val_acc.data)/total1)*100.0
  val_list.append(val_acc)
  loss_net /= total
  acc_net = (int(acc_net.data)/total)*100.0
  loss_list.append(loss_net)
  acc_list.append(acc_net)
  F1_list.append(F1)
  AUC_list.append(AUC)
  acc_list.append(acc)
  
  #print("Epoch:",e,"loss: ", loss_net, "acc:", acc_net, "Val Acc: "), 
  print("Epoch:",e,"loss: ", loss_net, "acc:", acc_net, "Val Acc: ", val_acc, "Val_loss: ", val_loss)

torch.save(model.state_dict(), "/content/gdrive/My Drive/Projects/COVID/model")

